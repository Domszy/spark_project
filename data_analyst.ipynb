{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/19 18:31:14 WARN Utils: Your hostname, Doomzies-2.local resolves to a loopback address: 127.0.0.1; using 192.168.68.101 instead (on interface en0)\n",
      "24/11/19 18:31:14 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/19 18:31:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/11/19 18:31:15 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|cpu_utilization|     event_datetime|free_memory|server_id|session_count|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|            0.8|04/08/2019 23:37:13|       0.15|      133|           68|\n",
      "|           0.84|04/08/2019 23:42:13|       0.14|      133|           66|\n",
      "|           0.69|04/08/2019 23:47:13|        0.4|      133|           72|\n",
      "|           0.75|04/08/2019 23:52:13|       0.18|      133|           77|\n",
      "|           0.63|04/08/2019 23:57:13|       0.16|      133|           82|\n",
      "|           0.72|04/09/2019 00:02:13|       0.06|      133|           78|\n",
      "|            0.9|04/09/2019 00:07:13|       0.37|      133|           68|\n",
      "|           0.59|04/09/2019 00:12:13|        0.4|      133|           94|\n",
      "|           0.87|04/09/2019 00:17:13|       0.35|      133|           86|\n",
      "|           0.83|04/09/2019 00:22:13|       0.34|      133|           92|\n",
      "|           0.71|04/09/2019 00:27:13|       0.38|      133|           94|\n",
      "|           0.64|04/09/2019 00:32:13|       0.15|      133|           69|\n",
      "|           0.82|04/09/2019 00:37:13|       0.16|      133|           88|\n",
      "|           0.83|04/09/2019 00:42:13|       0.15|      133|           90|\n",
      "|           0.69|04/09/2019 00:47:13|        0.1|      133|           96|\n",
      "|            0.9|04/09/2019 00:52:13|        0.2|      133|           66|\n",
      "|           0.66|04/09/2019 00:57:13|       0.13|      133|           77|\n",
      "|           0.94|04/09/2019 01:02:13|       0.09|      133|           91|\n",
      "|           0.84|04/09/2019 01:07:13|       0.26|      133|           84|\n",
      "|           0.71|04/09/2019 01:12:13|        0.4|      133|           67|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/19 18:31:30 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# instantiate spark driver node\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Spark data analyst\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# load csv file\n",
    "json_df2_path = './data/utilization.json'\n",
    "df = spark.read.format('json').load(json_df2_path)\n",
    "df.createOrReplaceTempView(\"utilization\")\n",
    "\n",
    "# print out dataframe\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/19 19:44:07 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 2:================================>                          (5 + 4) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-------------------+-------------------+------------------+------------------+\n",
      "|summary|    cpu_utilization|     event_datetime|        free_memory|         server_id|     session_count|\n",
      "+-------+-------------------+-------------------+-------------------+------------------+------------------+\n",
      "|  count|             500000|             500000|             500000|            500000|            500000|\n",
      "|   mean| 0.6205177400000123|               NULL|0.37912809999999764|             124.5|          69.59616|\n",
      "| stddev|0.15875173872912837|               NULL|0.15830931278376217|14.430884120553253|14.850676696352865|\n",
      "|    min|               0.22|03/05/2019 08:06:14|                0.0|               100|                32|\n",
      "|    max|                1.0|04/09/2019 01:22:46|               0.78|               149|               105|\n",
      "+-------+-------------------+-------------------+-------------------+------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# provides statistics on the features in df\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:================================>                          (5 + 4) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+-------------------+\n",
      "|summary|    cpu_utilization|        free_memory|\n",
      "+-------+-------------------+-------------------+\n",
      "|  count|             500000|             500000|\n",
      "|   mean| 0.6205177400000123|0.37912809999999764|\n",
      "| stddev|0.15875173872912837|0.15830931278376217|\n",
      "|    min|               0.22|                0.0|\n",
      "|    max|                1.0|               0.78|\n",
      "+-------+-------------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# chooses specific features to analyse\n",
    "df.describe('cpu_utilization', 'free_memory').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.4704771573080742"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# determine the correlation coefficient between cpu_utilization and free memory \n",
    "df.stat.corr('cpu_utilization', 'free_memory')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------------+\n",
      "| server_id_freqItems|session_count_freqItems|\n",
      "+--------------------+-----------------------+\n",
      "|[137, 146, 101, 1...|   [92, 101, 83, 104...|\n",
      "+--------------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# determine the frequent values for features \n",
    "df.stat.freqItems(('server_id', 'session_count')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "24870"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a sample\n",
    "df_util_sample = df.sample(fraction=0.05, withReplacement=False)\n",
    "df.createOrReplaceTempView(\"utilization_sample\")\n",
    "df_util_sample.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+--------------------+-----------------------+\n",
      "|server_id|min(cpu_utilization)|max(cpu_utilization)|stddev(cpu_utilization)|\n",
      "+---------+--------------------+--------------------+-----------------------+\n",
      "|      136|                0.41|                 0.8|    0.11597405743182258|\n",
      "|      139|                0.51|                0.91|    0.11519660023052102|\n",
      "|      138|                0.24|                0.64|     0.1155955860444133|\n",
      "|      142|                 0.5|                 0.9|    0.11593003726970043|\n",
      "|      141|                0.37|                0.77|    0.11521504481036793|\n",
      "|      134|                0.33|                0.73|    0.11566984462380653|\n",
      "|      140|                0.47|                0.87|    0.11539940805020545|\n",
      "|      137|                0.54|                0.94|    0.11526245077758812|\n",
      "|      135|                0.31|                0.71|    0.11512655070081647|\n",
      "|      133|                0.55|                0.95|    0.11534006553263144|\n",
      "|      112|                0.52|                0.92|    0.11528867845082576|\n",
      "|      113|                0.58|                0.98|    0.11544345150353694|\n",
      "|      119|                0.22|                0.62|    0.11516031929842005|\n",
      "|      116|                 0.3|                 0.7|    0.11506079722349302|\n",
      "|      114|                0.33|                0.73|    0.11510268816097273|\n",
      "|      115|                0.44|                0.84|    0.11569664615014985|\n",
      "|      111|                0.36|                0.76|    0.11530221569464483|\n",
      "|      118|                0.53|                0.93|    0.11474318421515234|\n",
      "|      117|                0.38|                0.78|    0.11534593941519553|\n",
      "|      107|                0.45|                0.85|    0.11597417369783877|\n",
      "+---------+--------------------+--------------------+-----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# GROUP BY server_id\n",
    "spark.sql('SELECT server_id, min(cpu_utilization), max(cpu_utilization), stddev(cpu_utilization) \\\n",
    "           FROM utilization \\\n",
    "           GROUP BY server_id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+\n",
      "|server_id|bucket|\n",
      "+---------+------+\n",
      "|      133|     8|\n",
      "|      133|     8|\n",
      "|      133|     6|\n",
      "|      133|     7|\n",
      "|      133|     6|\n",
      "|      133|     7|\n",
      "|      133|     9|\n",
      "|      133|     5|\n",
      "|      133|     8|\n",
      "|      133|     8|\n",
      "|      133|     7|\n",
      "|      133|     6|\n",
      "|      133|     8|\n",
      "|      133|     8|\n",
      "|      133|     6|\n",
      "|      133|     9|\n",
      "|      133|     6|\n",
      "|      133|     9|\n",
      "|      133|     8|\n",
      "|      133|     7|\n",
      "+---------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# FLoor function in SQL \n",
    "spark.sql(\n",
    "    'SELECT server_id, FLOOR(cpu_utilization*100/10) AS bucket FROM utilization').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+---------------+------------------+--------------------+\n",
      "|     event_datetime|server_id|cpu_utilization|   avg_server_util|   delta_server_util|\n",
      "+-------------------+---------+---------------+------------------+--------------------+\n",
      "|03/05/2019 08:06:31|      110|           0.68|0.5537749999999892|  0.1262250000000108|\n",
      "|03/05/2019 08:11:31|      110|           0.58|0.5537749999999892|0.026225000000010712|\n",
      "|03/05/2019 08:16:31|      110|           0.55|0.5537749999999892|-0.00377499999998...|\n",
      "|03/05/2019 08:21:31|      110|           0.63|0.5537749999999892| 0.07622500000001076|\n",
      "|03/05/2019 08:26:31|      110|           0.63|0.5537749999999892| 0.07622500000001076|\n",
      "|03/05/2019 08:31:31|      110|           0.71|0.5537749999999892| 0.15622500000001072|\n",
      "|03/05/2019 08:36:31|      110|           0.67|0.5537749999999892| 0.11622500000001079|\n",
      "|03/05/2019 08:41:31|      110|           0.55|0.5537749999999892|-0.00377499999998...|\n",
      "|03/05/2019 08:46:31|      110|           0.37|0.5537749999999892|-0.18377499999998925|\n",
      "|03/05/2019 08:51:31|      110|            0.7|0.5537749999999892|  0.1462250000000107|\n",
      "|03/05/2019 08:56:31|      110|           0.67|0.5537749999999892| 0.11622500000001079|\n",
      "|03/05/2019 09:01:31|      110|           0.56|0.5537749999999892|0.006225000000010805|\n",
      "|03/05/2019 09:06:31|      110|           0.37|0.5537749999999892|-0.18377499999998925|\n",
      "|03/05/2019 09:11:31|      110|            0.6|0.5537749999999892| 0.04622500000001073|\n",
      "|03/05/2019 09:16:31|      110|           0.45|0.5537749999999892|-0.10377499999998924|\n",
      "|03/05/2019 09:21:31|      110|           0.65|0.5537749999999892| 0.09622500000001077|\n",
      "|03/05/2019 09:26:31|      110|           0.45|0.5537749999999892|-0.10377499999998924|\n",
      "|03/05/2019 09:31:31|      110|           0.67|0.5537749999999892| 0.11622500000001079|\n",
      "|03/05/2019 09:36:31|      110|            0.5|0.5537749999999892|-0.05377499999998925|\n",
      "|03/05/2019 09:41:31|      110|           0.53|0.5537749999999892|-0.02377499999998922|\n",
      "+-------------------+---------+---------------+------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# determine the difference in avg cpu_utilization and each entry\n",
    "sql_window = \"SELECT event_datetime, server_id, cpu_utilization,  \\\n",
    "         avg(cpu_utilization) OVER (PARTITION BY server_id) AS avg_server_util, \\\n",
    "         cpu_utilization - avg(cpu_utilization) OVER (PARTITION BY server_id) AS delta_server_util \\\n",
    "         FROM utilization\"\n",
    "\n",
    "spark.sql(sql_window).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+---------------+---------------+\n",
      "|     event_datetime|server_id|cpu_utilization|avg_server_util|\n",
      "+-------------------+---------+---------------+---------------+\n",
      "|03/05/2019 08:06:31|      110|           0.68|           0.63|\n",
      "|03/05/2019 08:11:31|      110|           0.58|            0.6|\n",
      "|03/05/2019 08:16:31|      110|           0.55|           0.59|\n",
      "|03/05/2019 08:21:31|      110|           0.63|            0.6|\n",
      "|03/05/2019 08:26:31|      110|           0.63|           0.66|\n",
      "|03/05/2019 08:31:31|      110|           0.71|           0.67|\n",
      "|03/05/2019 08:36:31|      110|           0.67|           0.64|\n",
      "|03/05/2019 08:41:31|      110|           0.55|           0.53|\n",
      "|03/05/2019 08:46:31|      110|           0.37|           0.54|\n",
      "|03/05/2019 08:51:31|      110|            0.7|           0.58|\n",
      "|03/05/2019 08:56:31|      110|           0.67|           0.64|\n",
      "|03/05/2019 09:01:31|      110|           0.56|           0.53|\n",
      "|03/05/2019 09:06:31|      110|           0.37|           0.51|\n",
      "|03/05/2019 09:11:31|      110|            0.6|           0.47|\n",
      "|03/05/2019 09:16:31|      110|           0.45|           0.57|\n",
      "|03/05/2019 09:21:31|      110|           0.65|           0.52|\n",
      "|03/05/2019 09:26:31|      110|           0.45|           0.59|\n",
      "|03/05/2019 09:31:31|      110|           0.67|           0.54|\n",
      "|03/05/2019 09:36:31|      110|            0.5|           0.57|\n",
      "|03/05/2019 09:41:31|      110|           0.53|           0.51|\n",
      "+-------------------+---------+---------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# sliding window\n",
    "sql_window2 = \"SELECT event_datetime, server_id, cpu_utilization,  \\\n",
    "               ROUND(avg(cpu_utilization) OVER (PARTITION BY server_id ORDER BY event_datetime \\\n",
    "               ROWS BETWEEN 1 PRECEDING AND 1 FOLLOWING), 2) avg_server_util \\\n",
    "               FROM utilization\"\n",
    "\n",
    "spark.sql(sql_window2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "# take in the following features and output them as features in vector\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"cpu_utilization\", \"free_memory\", \"session_count\"], outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------------+-----------+---------+-------------+----------------+\n",
      "|cpu_utilization|     event_datetime|free_memory|server_id|session_count|        features|\n",
      "+---------------+-------------------+-----------+---------+-------------+----------------+\n",
      "|            0.8|04/08/2019 23:37:13|       0.15|      133|           68| [0.8,0.15,68.0]|\n",
      "|           0.84|04/08/2019 23:42:13|       0.14|      133|           66|[0.84,0.14,66.0]|\n",
      "|           0.69|04/08/2019 23:47:13|        0.4|      133|           72| [0.69,0.4,72.0]|\n",
      "|           0.75|04/08/2019 23:52:13|       0.18|      133|           77|[0.75,0.18,77.0]|\n",
      "|           0.63|04/08/2019 23:57:13|       0.16|      133|           82|[0.63,0.16,82.0]|\n",
      "|           0.72|04/09/2019 00:02:13|       0.06|      133|           78|[0.72,0.06,78.0]|\n",
      "|            0.9|04/09/2019 00:07:13|       0.37|      133|           68| [0.9,0.37,68.0]|\n",
      "|           0.59|04/09/2019 00:12:13|        0.4|      133|           94| [0.59,0.4,94.0]|\n",
      "|           0.87|04/09/2019 00:17:13|       0.35|      133|           86|[0.87,0.35,86.0]|\n",
      "|           0.83|04/09/2019 00:22:13|       0.34|      133|           92|[0.83,0.34,92.0]|\n",
      "|           0.71|04/09/2019 00:27:13|       0.38|      133|           94|[0.71,0.38,94.0]|\n",
      "|           0.64|04/09/2019 00:32:13|       0.15|      133|           69|[0.64,0.15,69.0]|\n",
      "|           0.82|04/09/2019 00:37:13|       0.16|      133|           88|[0.82,0.16,88.0]|\n",
      "|           0.83|04/09/2019 00:42:13|       0.15|      133|           90|[0.83,0.15,90.0]|\n",
      "|           0.69|04/09/2019 00:47:13|        0.1|      133|           96| [0.69,0.1,96.0]|\n",
      "|            0.9|04/09/2019 00:52:13|        0.2|      133|           66|  [0.9,0.2,66.0]|\n",
      "|           0.66|04/09/2019 00:57:13|       0.13|      133|           77|[0.66,0.13,77.0]|\n",
      "|           0.94|04/09/2019 01:02:13|       0.09|      133|           91|[0.94,0.09,91.0]|\n",
      "|           0.84|04/09/2019 01:07:13|       0.26|      133|           84|[0.84,0.26,84.0]|\n",
      "|           0.71|04/09/2019 01:12:13|        0.4|      133|           67| [0.71,0.4,67.0]|\n",
      "+---------------+-------------------+-----------+---------+-------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# transform the df by the assembler\n",
    "vcluster_df = assembler.transform(df)\n",
    "vcluster_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# kmeans configurations \n",
    "k_means = KMeans().setK(3)\n",
    "k_means = k_means.setSeed(1)\n",
    "\n",
    "# kmeans model \n",
    "kmeans_model = k_means.fit(vcluster_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 0.71174897,  0.28808911, 86.87510507]),\n",
       " array([ 0.61918113,  0.38080285, 68.75004716]),\n",
       " array([ 0.51439668,  0.48445202, 50.49452021])]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prints out kmeans clusters \n",
    "kmeans_model.clusterCenters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------------+-----------+---------+-------------+--------+\n",
      "|cpu_utilization|     event_datetime|free_memory|server_id|session_count|features|\n",
      "+---------------+-------------------+-----------+---------+-------------+--------+\n",
      "|            0.8|04/08/2019 23:37:13|       0.15|      133|           68|   [0.8]|\n",
      "|           0.84|04/08/2019 23:42:13|       0.14|      133|           66|  [0.84]|\n",
      "|           0.69|04/08/2019 23:47:13|        0.4|      133|           72|  [0.69]|\n",
      "|           0.75|04/08/2019 23:52:13|       0.18|      133|           77|  [0.75]|\n",
      "|           0.63|04/08/2019 23:57:13|       0.16|      133|           82|  [0.63]|\n",
      "|           0.72|04/09/2019 00:02:13|       0.06|      133|           78|  [0.72]|\n",
      "|            0.9|04/09/2019 00:07:13|       0.37|      133|           68|   [0.9]|\n",
      "|           0.59|04/09/2019 00:12:13|        0.4|      133|           94|  [0.59]|\n",
      "|           0.87|04/09/2019 00:17:13|       0.35|      133|           86|  [0.87]|\n",
      "|           0.83|04/09/2019 00:22:13|       0.34|      133|           92|  [0.83]|\n",
      "|           0.71|04/09/2019 00:27:13|       0.38|      133|           94|  [0.71]|\n",
      "|           0.64|04/09/2019 00:32:13|       0.15|      133|           69|  [0.64]|\n",
      "|           0.82|04/09/2019 00:37:13|       0.16|      133|           88|  [0.82]|\n",
      "|           0.83|04/09/2019 00:42:13|       0.15|      133|           90|  [0.83]|\n",
      "|           0.69|04/09/2019 00:47:13|        0.1|      133|           96|  [0.69]|\n",
      "|            0.9|04/09/2019 00:52:13|        0.2|      133|           66|   [0.9]|\n",
      "|           0.66|04/09/2019 00:57:13|       0.13|      133|           77|  [0.66]|\n",
      "|           0.94|04/09/2019 01:02:13|       0.09|      133|           91|  [0.94]|\n",
      "|           0.84|04/09/2019 01:07:13|       0.26|      133|           84|  [0.84]|\n",
      "|           0.71|04/09/2019 01:12:13|        0.4|      133|           67|  [0.71]|\n",
      "+---------------+-------------------+-----------+---------+-------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# linear regression\n",
    "linear_regression_assembler = VectorAssembler(\n",
    "    inputCols=[\"cpu_utilization\"], outputCol=\"features\")\n",
    "df_vutil = linear_regression_assembler.transform(df)\n",
    "df_vutil.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/19 20:30:01 WARN Instrumentation: [a6981fe1] regParam is zero, which might cause numerical instability and overfitting.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DenseVector([47.024])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate model\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"session_count\")\n",
    "lrModel = lr.fit(df_vutil)\n",
    "\n",
    "# determine the coefficient in linear regression\n",
    "lrModel.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40.41695103556927"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# determine the intercept\n",
    "lrModel.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.837990225931836"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# root mean squared error\n",
    "lrModel.summary.rootMeanSquaredError"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
