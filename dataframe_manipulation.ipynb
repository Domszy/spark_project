{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataframe Manipulation\n",
    "\n",
    "In this project, we will be learning about how we can instantiate a spark project, which we will use later in creating a End-to-end data pipeline. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading .CSV and .Json files\n",
    "\n",
    "\n",
    "### Instantiate SparkSession\n",
    "\n",
    "1. **Import PySpark**: Use `pyspark.sql` to leverage Spark's data processing capabilities.\n",
    "2. **Create a Spark Session**: `SparkSession.builder.getOrCreate()` initializes the driver node, which manages tasks across the cluster.\n",
    "3. **Purpose**: The `spark` object is the entry point for interacting with Spark for data analysis and processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/19 16:48:11 WARN Utils: Your hostname, Doomzies-2.local resolves to a loopback address: 127.0.0.1; using 192.168.68.101 instead (on interface en0)\n",
      "24/11/19 16:48:11 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/19 16:48:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.68.101:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x10d3db850>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import pyspark library\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# create driver node/\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and Displaying a CSV File in Spark\n",
    "\n",
    "1. **File Path**: Specify the file path (`location_temp_path`) for the CSV file to be loaded.\n",
    "2. **Reading Data**: \n",
    "   - Use `spark.read.format(\"csv\")` to load the CSV file.\n",
    "   - The `option(\"header\", \"true\")` ensures the first row is treated as column headers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+------------+\n",
      "|         event_date|location_id|temp_celcius|\n",
      "+-------------------+-----------+------------+\n",
      "|03/04/2019 19:48:06|       loc0|          29|\n",
      "|03/04/2019 19:53:06|       loc0|          27|\n",
      "|03/04/2019 19:58:06|       loc0|          28|\n",
      "|03/04/2019 20:03:06|       loc0|          30|\n",
      "|03/04/2019 20:08:06|       loc0|          27|\n",
      "|03/04/2019 20:13:06|       loc0|          27|\n",
      "|03/04/2019 20:18:06|       loc0|          27|\n",
      "|03/04/2019 20:23:06|       loc0|          29|\n",
      "|03/04/2019 20:28:06|       loc0|          32|\n",
      "|03/04/2019 20:33:06|       loc0|          35|\n",
      "|03/04/2019 20:38:06|       loc0|          32|\n",
      "|03/04/2019 20:43:06|       loc0|          28|\n",
      "|03/04/2019 20:48:06|       loc0|          28|\n",
      "|03/04/2019 20:53:06|       loc0|          32|\n",
      "|03/04/2019 20:58:06|       loc0|          34|\n",
      "|03/04/2019 21:03:06|       loc0|          33|\n",
      "|03/04/2019 21:08:06|       loc0|          27|\n",
      "|03/04/2019 21:13:06|       loc0|          28|\n",
      "|03/04/2019 21:18:06|       loc0|          33|\n",
      "|03/04/2019 21:23:06|       loc0|          28|\n",
      "+-------------------+-----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load location_temp.csv to df\n",
    "location_temp_path = './data/location_temp.csv'\n",
    "df_1 = spark.read.format(\"csv\").option(\n",
    "    \"header\", \"true\").load(location_temp_path)  # specify the format to be either 'csv' or 'json'\n",
    "\n",
    "df_1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---+----+----+---+\n",
      "|                _c0|_c1| _c2| _c3|_c4|\n",
      "+-------------------+---+----+----+---+\n",
      "|03/05/2019 08:06:14|100|0.57|0.51| 47|\n",
      "|03/05/2019 08:11:14|100|0.47|0.62| 43|\n",
      "|03/05/2019 08:16:14|100|0.56|0.57| 62|\n",
      "|03/05/2019 08:21:14|100|0.57|0.56| 50|\n",
      "|03/05/2019 08:26:14|100|0.35|0.46| 43|\n",
      "|03/05/2019 08:31:14|100|0.41|0.58| 48|\n",
      "|03/05/2019 08:36:14|100|0.57|0.35| 58|\n",
      "|03/05/2019 08:41:14|100|0.41| 0.4| 58|\n",
      "|03/05/2019 08:46:14|100|0.53|0.35| 62|\n",
      "|03/05/2019 08:51:14|100|0.51| 0.6| 45|\n",
      "|03/05/2019 08:56:14|100|0.32|0.37| 47|\n",
      "|03/05/2019 09:01:14|100|0.62|0.59| 60|\n",
      "|03/05/2019 09:06:14|100|0.66|0.72| 57|\n",
      "|03/05/2019 09:11:14|100|0.54|0.54| 44|\n",
      "|03/05/2019 09:16:14|100|0.29| 0.4| 47|\n",
      "|03/05/2019 09:21:14|100|0.43|0.68| 66|\n",
      "|03/05/2019 09:26:14|100|0.49|0.66| 65|\n",
      "|03/05/2019 09:31:14|100|0.64|0.55| 66|\n",
      "|03/05/2019 09:36:14|100|0.42| 0.6| 42|\n",
      "|03/05/2019 09:41:14|100|0.55|0.59| 63|\n",
      "+-------------------+---+----+----+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load utilization.csv into df\n",
    "utilization_path = \"./data//utilization.csv\"\n",
    "df_2 = spark.read.format(\"csv\").option(\"header\", \"false\").option(\n",
    "    \"inferSchema\", \"true\").load(utilization_path)\n",
    "\n",
    "df_2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will be comparing the count and the columns found in both data frames. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(event_date='03/04/2019 19:48:06', location_id='loc0', temp_celcius='29'),\n",
       " Row(event_date='03/04/2019 19:53:06', location_id='loc0', temp_celcius='27'),\n",
       " Row(event_date='03/04/2019 19:58:06', location_id='loc0', temp_celcius='28'),\n",
       " Row(event_date='03/04/2019 20:03:06', location_id='loc0', temp_celcius='30'),\n",
       " Row(event_date='03/04/2019 20:08:06', location_id='loc0', temp_celcius='27'),\n",
       " Row(event_date='03/04/2019 20:13:06', location_id='loc0', temp_celcius='27'),\n",
       " Row(event_date='03/04/2019 20:18:06', location_id='loc0', temp_celcius='27'),\n",
       " Row(event_date='03/04/2019 20:23:06', location_id='loc0', temp_celcius='29'),\n",
       " Row(event_date='03/04/2019 20:28:06', location_id='loc0', temp_celcius='32'),\n",
       " Row(event_date='03/04/2019 20:33:06', location_id='loc0', temp_celcius='35')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show the first 10 entries in csv file\n",
    "df_1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# determine the number of entries in csv file\n",
    "df_1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_c0='03/05/2019 08:06:14', _c1=100, _c2=0.57, _c3=0.51, _c4=47),\n",
       " Row(_c0='03/05/2019 08:11:14', _c1=100, _c2=0.47, _c3=0.62, _c4=43),\n",
       " Row(_c0='03/05/2019 08:16:14', _c1=100, _c2=0.56, _c3=0.57, _c4=62),\n",
       " Row(_c0='03/05/2019 08:21:14', _c1=100, _c2=0.57, _c3=0.56, _c4=50),\n",
       " Row(_c0='03/05/2019 08:26:14', _c1=100, _c2=0.35, _c3=0.46, _c4=43),\n",
       " Row(_c0='03/05/2019 08:31:14', _c1=100, _c2=0.41, _c3=0.58, _c4=48),\n",
       " Row(_c0='03/05/2019 08:36:14', _c1=100, _c2=0.57, _c3=0.35, _c4=58),\n",
       " Row(_c0='03/05/2019 08:41:14', _c1=100, _c2=0.41, _c3=0.4, _c4=58),\n",
       " Row(_c0='03/05/2019 08:46:14', _c1=100, _c2=0.53, _c3=0.35, _c4=62),\n",
       " Row(_c0='03/05/2019 08:51:14', _c1=100, _c2=0.51, _c3=0.6, _c4=45)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/19 16:48:23 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "500000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be renaming the columns in `df_2` to be as such: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = df_2.withColumnRenamed(\"_c0\", \"event_datetime\") \\\n",
    "    .withColumnRenamed(\"_c1\", \"server_id\")       \\\n",
    "    .withColumnRenamed(\"_c2\", \"cpu_utilization\")  \\\n",
    "    .withColumnRenamed(\"_c3\", \"free_memory\")      \\\n",
    "    .withColumnRenamed(\"_c4\", \"session_count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+---------------+-----------+-------------+\n",
      "|     event_datetime|server_id|cpu_utilization|free_memory|session_count|\n",
      "+-------------------+---------+---------------+-----------+-------------+\n",
      "|03/05/2019 08:06:14|      100|           0.57|       0.51|           47|\n",
      "|03/05/2019 08:11:14|      100|           0.47|       0.62|           43|\n",
      "|03/05/2019 08:16:14|      100|           0.56|       0.57|           62|\n",
      "|03/05/2019 08:21:14|      100|           0.57|       0.56|           50|\n",
      "|03/05/2019 08:26:14|      100|           0.35|       0.46|           43|\n",
      "|03/05/2019 08:31:14|      100|           0.41|       0.58|           48|\n",
      "|03/05/2019 08:36:14|      100|           0.57|       0.35|           58|\n",
      "|03/05/2019 08:41:14|      100|           0.41|        0.4|           58|\n",
      "|03/05/2019 08:46:14|      100|           0.53|       0.35|           62|\n",
      "|03/05/2019 08:51:14|      100|           0.51|        0.6|           45|\n",
      "|03/05/2019 08:56:14|      100|           0.32|       0.37|           47|\n",
      "|03/05/2019 09:01:14|      100|           0.62|       0.59|           60|\n",
      "|03/05/2019 09:06:14|      100|           0.66|       0.72|           57|\n",
      "|03/05/2019 09:11:14|      100|           0.54|       0.54|           44|\n",
      "|03/05/2019 09:16:14|      100|           0.29|        0.4|           47|\n",
      "|03/05/2019 09:21:14|      100|           0.43|       0.68|           66|\n",
      "|03/05/2019 09:26:14|      100|           0.49|       0.66|           65|\n",
      "|03/05/2019 09:31:14|      100|           0.64|       0.55|           66|\n",
      "|03/05/2019 09:36:14|      100|           0.42|        0.6|           42|\n",
      "|03/05/2019 09:41:14|      100|           0.55|       0.59|           63|\n",
      "+-------------------+---------+---------------+-----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will be saving our files as `.json` files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "json_df1_path = \"./data/location_temp.json\"\n",
    "df_1.write.json(json_df1_path)\n",
    "\n",
    "json_df2_path = \"./data/utilization.json\"\n",
    "df_2.write.json(json_df2_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be doing the same, except now we will be using `.json` files instead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:========================>                                 (3 + 4) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+------------+\n",
      "|         event_date|location_id|temp_celcius|\n",
      "+-------------------+-----------+------------+\n",
      "|03/04/2019 19:48:06|       loc0|          29|\n",
      "|03/04/2019 19:53:06|       loc0|          27|\n",
      "|03/04/2019 19:58:06|       loc0|          28|\n",
      "|03/04/2019 20:03:06|       loc0|          30|\n",
      "|03/04/2019 20:08:06|       loc0|          27|\n",
      "|03/04/2019 20:13:06|       loc0|          27|\n",
      "|03/04/2019 20:18:06|       loc0|          27|\n",
      "|03/04/2019 20:23:06|       loc0|          29|\n",
      "|03/04/2019 20:28:06|       loc0|          32|\n",
      "|03/04/2019 20:33:06|       loc0|          35|\n",
      "|03/04/2019 20:38:06|       loc0|          32|\n",
      "|03/04/2019 20:43:06|       loc0|          28|\n",
      "|03/04/2019 20:48:06|       loc0|          28|\n",
      "|03/04/2019 20:53:06|       loc0|          32|\n",
      "|03/04/2019 20:58:06|       loc0|          34|\n",
      "|03/04/2019 21:03:06|       loc0|          33|\n",
      "|03/04/2019 21:08:06|       loc0|          27|\n",
      "|03/04/2019 21:13:06|       loc0|          28|\n",
      "|03/04/2019 21:18:06|       loc0|          33|\n",
      "|03/04/2019 21:23:06|       loc0|          28|\n",
      "+-------------------+-----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_1_json = spark.read.format('json').load(json_df1_path)\n",
    "df_1_json.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic DataFrame Operations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['event_datetime',\n",
       " 'server_id',\n",
       " 'cpu_utilization',\n",
       " 'free_memory',\n",
       " 'session_count']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print out columns in dataframe\n",
    "df_2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+---------------+-----------+-------------+\n",
      "|     event_datetime|server_id|cpu_utilization|free_memory|session_count|\n",
      "+-------------------+---------+---------------+-----------+-------------+\n",
      "|03/05/2019 08:11:14|      100|           0.47|       0.62|           43|\n",
      "|03/05/2019 08:21:14|      100|           0.57|       0.56|           50|\n",
      "|03/05/2019 09:06:14|      100|           0.66|       0.72|           57|\n",
      "|03/05/2019 10:36:14|      100|           0.62|       0.36|           50|\n",
      "|03/05/2019 15:56:14|      100|           0.63|       0.42|           61|\n",
      "|03/05/2019 17:26:14|      100|           0.34|       0.61|           52|\n",
      "|03/05/2019 18:26:14|      100|           0.44|       0.52|           72|\n",
      "|03/05/2019 18:31:14|      100|           0.28|       0.72|           57|\n",
      "|03/05/2019 18:56:14|      100|           0.52|       0.66|           47|\n",
      "|03/05/2019 19:01:14|      100|           0.61|       0.45|           38|\n",
      "|03/05/2019 20:21:14|      100|           0.32|       0.41|           66|\n",
      "|03/05/2019 20:41:14|      100|           0.66|       0.64|           66|\n",
      "|03/05/2019 21:46:14|      100|           0.52|       0.49|           53|\n",
      "|03/05/2019 22:01:14|      100|           0.65|       0.44|           67|\n",
      "|03/05/2019 23:06:14|      100|           0.55|       0.33|           66|\n",
      "|03/06/2019 00:16:14|      100|           0.34|       0.57|           49|\n",
      "|03/06/2019 01:46:14|      100|           0.34|       0.55|           71|\n",
      "|03/06/2019 01:56:14|      100|           0.51|       0.38|           53|\n",
      "|03/06/2019 02:11:14|      100|           0.51|       0.67|           65|\n",
      "|03/06/2019 02:16:14|      100|           0.57|       0.48|           47|\n",
      "+-------------------+---------+---------------+-----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sampling from the dataframe\n",
    "# the boolean specifies if we want to replace the entry that has been sampled in subsequent sampling\n",
    "# fraction specifies the percentage of entries we want\n",
    "df_2_sample = df_2.sample(False, fraction=0.1)\n",
    "\n",
    "df_2_sample.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+---------------+-----------+-------------+\n",
      "|     event_datetime|server_id|cpu_utilization|free_memory|session_count|\n",
      "+-------------------+---------+---------------+-----------+-------------+\n",
      "|03/05/2019 08:06:21|      104|           0.84|       0.36|           94|\n",
      "|03/05/2019 08:06:26|      107|           0.64|       0.55|           56|\n",
      "|03/05/2019 08:06:28|      108|           0.62|       0.41|           86|\n",
      "|03/05/2019 08:06:46|      119|           0.51|       0.49|           53|\n",
      "|03/05/2019 08:06:56|      125|           0.42|       0.37|           58|\n",
      "|03/05/2019 08:07:06|      130|           0.52|       0.49|           77|\n",
      "|03/05/2019 08:07:15|      135|           0.55|        0.5|           70|\n",
      "|03/05/2019 08:11:14|      100|           0.47|       0.62|           43|\n",
      "|03/05/2019 08:11:16|      101|           0.71|       0.18|           98|\n",
      "|03/05/2019 08:11:17|      102|           0.59|       0.13|           70|\n",
      "|03/05/2019 08:11:26|      107|           0.83|        0.2|           72|\n",
      "|03/05/2019 08:12:44|      149|            0.9|       0.34|           85|\n",
      "|03/05/2019 08:16:17|      102|           0.91|       0.33|           75|\n",
      "|03/05/2019 08:16:29|      109|            0.7|       0.45|           71|\n",
      "|03/05/2019 08:17:03|      129|           0.55|       0.55|           48|\n",
      "|03/05/2019 08:21:14|      100|           0.57|       0.56|           50|\n",
      "|03/05/2019 08:21:21|      104|            0.8|       0.36|           94|\n",
      "|03/05/2019 08:21:28|      108|           0.69|       0.44|           68|\n",
      "|03/05/2019 08:21:31|      110|           0.63|       0.39|           70|\n",
      "|03/05/2019 08:21:38|      114|           0.62|       0.57|           74|\n",
      "+-------------------+---------+---------------+-----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# sort by a feature\n",
    "df_2_sorted = df_2_sample.sort('event_datetime')\n",
    "\n",
    "df_2_sorted.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+------------+\n",
      "|         event_date|location_id|temp_celcius|\n",
      "+-------------------+-----------+------------+\n",
      "|03/04/2019 19:48:06|       loc0|          29|\n",
      "|03/04/2019 19:53:06|       loc0|          27|\n",
      "|03/04/2019 19:58:06|       loc0|          28|\n",
      "|03/04/2019 20:03:06|       loc0|          30|\n",
      "|03/04/2019 20:08:06|       loc0|          27|\n",
      "|03/04/2019 20:13:06|       loc0|          27|\n",
      "|03/04/2019 20:18:06|       loc0|          27|\n",
      "|03/04/2019 20:23:06|       loc0|          29|\n",
      "|03/04/2019 20:28:06|       loc0|          32|\n",
      "|03/04/2019 20:33:06|       loc0|          35|\n",
      "|03/04/2019 20:38:06|       loc0|          32|\n",
      "|03/04/2019 20:43:06|       loc0|          28|\n",
      "|03/04/2019 20:48:06|       loc0|          28|\n",
      "|03/04/2019 20:53:06|       loc0|          32|\n",
      "|03/04/2019 20:58:06|       loc0|          34|\n",
      "|03/04/2019 21:03:06|       loc0|          33|\n",
      "|03/04/2019 21:08:06|       loc0|          27|\n",
      "|03/04/2019 21:13:06|       loc0|          28|\n",
      "|03/04/2019 21:18:06|       loc0|          33|\n",
      "|03/04/2019 21:23:06|       loc0|          28|\n",
      "+-------------------+-----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filtering data in dataframe\n",
    "df_1.filter(df_1[\"location_id\"] == \"loc0\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 21:>                                                         (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|location_id|count|\n",
      "+-----------+-----+\n",
      "|      loc22| 1000|\n",
      "|      loc31| 1000|\n",
      "|      loc82| 1000|\n",
      "|      loc90| 1000|\n",
      "|     loc118| 1000|\n",
      "|      loc39| 1000|\n",
      "|      loc75| 1000|\n",
      "|     loc122| 1000|\n",
      "|      loc24| 1000|\n",
      "|      loc30| 1000|\n",
      "|     loc105| 1000|\n",
      "|      loc96| 1000|\n",
      "|     loc102| 1000|\n",
      "|      loc18| 1000|\n",
      "|      loc27| 1000|\n",
      "|     loc143| 1000|\n",
      "|      loc43| 1000|\n",
      "|     loc123| 1000|\n",
      "|      loc15| 1000|\n",
      "|      loc48| 1000|\n",
      "+-----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Aggregating data\n",
    "\n",
    "# group by category, and attaining count \n",
    "df_1.groupBy(\"location_id\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+------------+\n",
      "|         event_date|location_id|temp_celcius|\n",
      "+-------------------+-----------+------------+\n",
      "|03/04/2019 19:48:06|       loc0|          29|\n",
      "|03/04/2019 19:53:06|       loc0|          27|\n",
      "|03/04/2019 19:58:06|       loc0|          28|\n",
      "|03/04/2019 20:03:06|       loc0|          30|\n",
      "|03/04/2019 20:08:06|       loc0|          27|\n",
      "|03/04/2019 20:13:06|       loc0|          27|\n",
      "|03/04/2019 20:18:06|       loc0|          27|\n",
      "|03/04/2019 20:23:06|       loc0|          29|\n",
      "|03/04/2019 20:28:06|       loc0|          32|\n",
      "|03/04/2019 20:33:06|       loc0|          35|\n",
      "+-------------------+-----------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ordering\n",
    "df_1.orderBy(\"location_id\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------+\n",
      "|location_id|avg(temp_celcius)|\n",
      "+-----------+-----------------+\n",
      "|      loc22|           28.251|\n",
      "|      loc31|           25.196|\n",
      "|      loc82|           27.355|\n",
      "|      loc90|           23.216|\n",
      "|     loc118|           24.219|\n",
      "|      loc39|           25.199|\n",
      "|      loc75|           23.209|\n",
      "|     loc122|            32.36|\n",
      "|      loc24|           31.109|\n",
      "|      loc30|           30.211|\n",
      "|     loc105|           26.217|\n",
      "|      loc96|           28.138|\n",
      "|     loc102|           30.327|\n",
      "|      loc18|           30.198|\n",
      "|      loc27|           31.239|\n",
      "|     loc143|           28.213|\n",
      "|      loc43|           32.196|\n",
      "|     loc123|           23.424|\n",
      "|      loc15|           32.171|\n",
      "|      loc48|           30.244|\n",
      "+-----------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# finding average\n",
    "df_1.groupBy(\"location_id\").agg({'temp_celcius': 'mean'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------+\n",
      "|location_id|max(temp_celcius)|\n",
      "+-----------+-----------------+\n",
      "|       loc0|               36|\n",
      "|       loc1|               35|\n",
      "|      loc10|               32|\n",
      "|     loc100|               34|\n",
      "|     loc101|               32|\n",
      "|     loc102|               37|\n",
      "|     loc103|               32|\n",
      "|     loc104|               33|\n",
      "|     loc105|               33|\n",
      "|     loc106|               34|\n",
      "|     loc107|               40|\n",
      "|     loc108|               39|\n",
      "|     loc109|               31|\n",
      "|      loc11|               32|\n",
      "|     loc110|               33|\n",
      "|     loc111|               38|\n",
      "|     loc112|               40|\n",
      "|     loc113|               37|\n",
      "|     loc114|               36|\n",
      "|     loc115|               30|\n",
      "+-----------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# find maximum\n",
    "df_1.groupby('location_id').agg({'temp_celcius': 'max'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50040"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_s1 = df_1.sample(fraction=0.1, withReplacement=False)\n",
    "df_s1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n",
      "|location_id| avg(temp_celcius)|\n",
      "+-----------+------------------+\n",
      "|      loc22| 27.96590909090909|\n",
      "|      loc31|25.207207207207208|\n",
      "|      loc82|27.233644859813083|\n",
      "|      loc90| 23.16504854368932|\n",
      "|     loc118| 24.30275229357798|\n",
      "|      loc39| 25.27777777777778|\n",
      "|      loc75|23.341463414634145|\n",
      "|     loc122| 32.31683168316832|\n",
      "|      loc24|             31.25|\n",
      "|      loc30|30.214285714285715|\n",
      "+-----------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_s1.groupBy(\"location_id\").agg({'temp_celcius': 'mean'}).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n",
      "|location_id| avg(temp_celcius)|\n",
      "+-----------+------------------+\n",
      "|       loc0| 29.59259259259259|\n",
      "|       loc1|28.425742574257427|\n",
      "|      loc10|25.383838383838384|\n",
      "|     loc100|27.540816326530614|\n",
      "|     loc101|25.515151515151516|\n",
      "|     loc102|30.145299145299145|\n",
      "|     loc103|25.326923076923077|\n",
      "|     loc104| 25.91860465116279|\n",
      "|     loc105| 26.23469387755102|\n",
      "|     loc106|26.813725490196077|\n",
      "+-----------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_s1.groupBy(\"location_id\").agg(\n",
    "    {'temp_celcius': 'mean'}).orderBy(\"location_id\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------+\n",
      "|location_id|avg(temp_celcius)|\n",
      "+-----------+-----------------+\n",
      "|       loc0|           29.176|\n",
      "|       loc1|           28.246|\n",
      "|      loc10|           25.337|\n",
      "|     loc100|           27.297|\n",
      "|     loc101|           25.317|\n",
      "|     loc102|           30.327|\n",
      "|     loc103|           25.341|\n",
      "|     loc104|           26.204|\n",
      "|     loc105|           26.217|\n",
      "|     loc106|           27.201|\n",
      "+-----------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_1.groupBy(\"location_id\").agg(\n",
    "    {'temp_celcius': 'mean'}).orderBy(\"location_id\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saves dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_1.write.csv('./data/df1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_SUCCESS\n",
      "part-00000-ad92fee7-c79c-4887-ad74-6f41adfeca67-c000.csv\n",
      "part-00001-ad92fee7-c79c-4887-ad74-6f41adfeca67-c000.csv\n",
      "part-00002-ad92fee7-c79c-4887-ad74-6f41adfeca67-c000.csv\n",
      "part-00003-ad92fee7-c79c-4887-ad74-6f41adfeca67-c000.csv\n"
     ]
    }
   ],
   "source": [
    "! ls data/df1.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/04/2019 19:48:06,loc0,29\n",
      "03/04/2019 19:53:06,loc0,27\n",
      "03/04/2019 19:58:06,loc0,28\n",
      "03/04/2019 20:03:06,loc0,30\n",
      "03/04/2019 20:08:06,loc0,27\n",
      "03/04/2019 20:13:06,loc0,27\n",
      "03/04/2019 20:18:06,loc0,27\n",
      "03/04/2019 20:23:06,loc0,29\n",
      "03/04/2019 20:28:06,loc0,32\n",
      "03/04/2019 20:33:06,loc0,35\n"
     ]
    }
   ],
   "source": [
    "! head data/df1.csv/part-00000-ad92fee7-c79c-4887-ad74-6f41adfeca67-c000.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
