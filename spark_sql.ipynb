{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataframes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/19 17:40:05 WARN Utils: Your hostname, Doomzies-2.local resolves to a loopback address: 127.0.0.1; using 192.168.68.101 instead (on interface en0)\n",
      "24/11/19 17:40:05 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/19 17:40:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/11/19 17:40:07 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|cpu_utilization|     event_datetime|free_memory|server_id|session_count|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|            0.8|04/08/2019 23:37:13|       0.15|      133|           68|\n",
      "|           0.84|04/08/2019 23:42:13|       0.14|      133|           66|\n",
      "|           0.69|04/08/2019 23:47:13|        0.4|      133|           72|\n",
      "|           0.75|04/08/2019 23:52:13|       0.18|      133|           77|\n",
      "|           0.63|04/08/2019 23:57:13|       0.16|      133|           82|\n",
      "|           0.72|04/09/2019 00:02:13|       0.06|      133|           78|\n",
      "|            0.9|04/09/2019 00:07:13|       0.37|      133|           68|\n",
      "|           0.59|04/09/2019 00:12:13|        0.4|      133|           94|\n",
      "|           0.87|04/09/2019 00:17:13|       0.35|      133|           86|\n",
      "|           0.83|04/09/2019 00:22:13|       0.34|      133|           92|\n",
      "|           0.71|04/09/2019 00:27:13|       0.38|      133|           94|\n",
      "|           0.64|04/09/2019 00:32:13|       0.15|      133|           69|\n",
      "|           0.82|04/09/2019 00:37:13|       0.16|      133|           88|\n",
      "|           0.83|04/09/2019 00:42:13|       0.15|      133|           90|\n",
      "|           0.69|04/09/2019 00:47:13|        0.1|      133|           96|\n",
      "|            0.9|04/09/2019 00:52:13|        0.2|      133|           66|\n",
      "|           0.66|04/09/2019 00:57:13|       0.13|      133|           77|\n",
      "|           0.94|04/09/2019 01:02:13|       0.09|      133|           91|\n",
      "|           0.84|04/09/2019 01:07:13|       0.26|      133|           84|\n",
      "|           0.71|04/09/2019 01:12:13|        0.4|      133|           67|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/19 17:40:19 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# instantiate spark driver node\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Spark SQL Query Dataframes\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# load csv file\n",
    "json_df2_path = './data/utilization.json'\n",
    "df = spark.read.format('json').load(json_df2_path)\n",
    "\n",
    "# print out dataframe\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "500000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# determine count of rows in data frame\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cpu_utilization: double (nullable = true)\n",
      " |-- event_datetime: string (nullable = true)\n",
      " |-- free_memory: double (nullable = true)\n",
      " |-- server_id: long (nullable = true)\n",
      " |-- session_count: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# prints the schema of table out\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running SQL commands on table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|cpu_utilization|     event_datetime|free_memory|server_id|session_count|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|            0.8|04/08/2019 23:37:13|       0.15|      133|           68|\n",
      "|           0.84|04/08/2019 23:42:13|       0.14|      133|           66|\n",
      "|           0.69|04/08/2019 23:47:13|        0.4|      133|           72|\n",
      "|           0.75|04/08/2019 23:52:13|       0.18|      133|           77|\n",
      "|           0.63|04/08/2019 23:57:13|       0.16|      133|           82|\n",
      "|           0.72|04/09/2019 00:02:13|       0.06|      133|           78|\n",
      "|            0.9|04/09/2019 00:07:13|       0.37|      133|           68|\n",
      "|           0.59|04/09/2019 00:12:13|        0.4|      133|           94|\n",
      "|           0.87|04/09/2019 00:17:13|       0.35|      133|           86|\n",
      "|           0.83|04/09/2019 00:22:13|       0.34|      133|           92|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# rename the table to 'utilization'\n",
    "df.createOrReplaceTempView(\"utilization\")\n",
    "\n",
    "# apply SQL query on table utilization\n",
    "df_sql = spark.sql(\"SELECT * FROM utilization LIMIT 10\")\n",
    "df_sql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|sid| sc|\n",
      "+---+---+\n",
      "|133| 68|\n",
      "|133| 66|\n",
      "|133| 72|\n",
      "|133| 77|\n",
      "|133| 82|\n",
      "|133| 78|\n",
      "|133| 68|\n",
      "|133| 94|\n",
      "|133| 86|\n",
      "|133| 92|\n",
      "|133| 94|\n",
      "|133| 69|\n",
      "|133| 88|\n",
      "|133| 90|\n",
      "|133| 96|\n",
      "|133| 66|\n",
      "|133| 77|\n",
      "|133| 91|\n",
      "|133| 84|\n",
      "|133| 67|\n",
      "+---+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# selecting specific features\n",
    "df_sql = spark.sql(\n",
    "    \"SELECT server_id as sid, session_count as sc FROM utilization\")\n",
    "df_sql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|cpu_utilization|     event_datetime|free_memory|server_id|session_count|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "|            0.6|03/06/2019 08:26:48|       0.39|      120|           80|\n",
      "|           0.72|03/07/2019 18:21:48|       0.46|      120|           80|\n",
      "|           0.65|03/06/2019 08:51:48|       0.31|      120|           80|\n",
      "|           0.58|03/05/2019 19:16:48|       0.56|      120|           80|\n",
      "|           0.44|03/06/2019 10:01:48|       0.41|      120|           80|\n",
      "|            0.7|03/05/2019 08:21:48|       0.35|      120|           80|\n",
      "|           0.65|03/06/2019 10:56:48|       0.45|      120|           80|\n",
      "|           0.72|03/05/2019 23:26:48|       0.64|      120|           80|\n",
      "|           0.61|03/06/2019 11:31:48|       0.25|      120|           80|\n",
      "|           0.71|03/06/2019 23:56:48|       0.63|      120|           80|\n",
      "|           0.66|03/07/2019 00:51:48|        0.6|      120|           80|\n",
      "|           0.74|03/05/2019 19:26:48|       0.55|      120|           80|\n",
      "|           0.64|03/07/2019 01:21:48|       0.32|      120|           80|\n",
      "|           0.59|03/05/2019 22:26:48|       0.57|      120|           80|\n",
      "|            0.4|03/07/2019 07:51:48|        0.4|      120|           80|\n",
      "|           0.73|03/06/2019 16:56:48|       0.36|      120|           80|\n",
      "|            0.6|03/07/2019 09:41:48|       0.36|      120|           80|\n",
      "|           0.53|03/07/2019 15:16:48|       0.56|      120|           80|\n",
      "|            0.5|03/07/2019 10:16:48|       0.31|      120|           80|\n",
      "|           0.46|03/06/2019 04:21:48|       0.55|      120|           80|\n",
      "+---------------+-------------------+-----------+---------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filtering for SQL using WHERE clause\n",
    "df_sql = spark.sql(\"SELECT * \\\n",
    "                   FROM utilization \\\n",
    "                   WHERE session_count > 70 AND server_id = 120 \\\n",
    "                   ORDER BY session_count DESC\")\n",
    "df_sql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|  239659|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# aggregate functions\n",
    "\n",
    "# determine count of entries where session_count > 70\n",
    "df_sql = spark.sql(\"SELECT count(*) \\\n",
    "                    FROM utilization \\\n",
    "                    WHERE session_count > 70\")\n",
    "df_sql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+----------------------------+------------------+\n",
      "|server_id|min(session_count)|round(avg(session_count), 2)|max(session_count)|\n",
      "+---------+------------------+----------------------------+------------------+\n",
      "|      101|                71|                       87.67|               105|\n",
      "|      113|                71|                       86.96|               103|\n",
      "|      145|                71|                       86.98|               103|\n",
      "|      103|                71|                       85.76|               101|\n",
      "|      102|                71|                       85.71|               101|\n",
      "|      133|                71|                       85.47|               100|\n",
      "|      108|                71|                       85.12|               100|\n",
      "|      149|                71|                       84.96|                99|\n",
      "|      137|                71|                       85.01|                99|\n",
      "|      148|                71|                        84.7|                99|\n",
      "|      123|                71|                       84.53|                98|\n",
      "|      118|                71|                       84.66|                98|\n",
      "|      112|                71|                       83.55|                97|\n",
      "|      139|                71|                       83.33|                96|\n",
      "|      104|                71|                       83.35|                96|\n",
      "|      142|                71|                        82.9|                95|\n",
      "|      121|                71|                       82.89|                95|\n",
      "|      146|                71|                       82.95|                95|\n",
      "|      126|                71|                       81.56|                93|\n",
      "|      144|                71|                       81.38|                92|\n",
      "+---------+------------------+----------------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# complex aggregate function\n",
    "df_sql = spark.sql(\"SELECT server_id, min(session_count), round(avg(session_count),2), max(session_count) \\\n",
    "                    FROM utilization \\\n",
    "                    WHERE session_count > 70 \\\n",
    "                    GROUP BY server_id \\\n",
    "                    ORDER BY count(*) DESC\")\n",
    "df_sql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|server_id|server_name|\n",
      "+---------+-----------+\n",
      "|      100| 100 Server|\n",
      "|      101| 101 Server|\n",
      "|      102| 102 Server|\n",
      "|      103| 103 Server|\n",
      "|      104| 104 Server|\n",
      "|      105| 105 Server|\n",
      "|      106| 106 Server|\n",
      "|      107| 107 Server|\n",
      "|      108| 108 Server|\n",
      "|      109| 109 Server|\n",
      "|      110| 110 Server|\n",
      "|      111| 111 Server|\n",
      "|      112| 112 Server|\n",
      "|      113| 113 Server|\n",
      "|      114| 114 Server|\n",
      "|      115| 115 Server|\n",
      "|      116| 116 Server|\n",
      "|      117| 117 Server|\n",
      "|      118| 118 Server|\n",
      "|      119| 119 Server|\n",
      "+---------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Joining two tables together\n",
    "\n",
    "# create new table\n",
    "csv_df_path = \"./data/server_name.csv\"\n",
    "df_server = spark.read.csv(csv_df_path, header=True)\n",
    "\n",
    "# rename table to server_name in SQL\n",
    "df_server.createOrReplaceTempView(\"server_name\")\n",
    "\n",
    "# print table\n",
    "df_server.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 26:===================>                                      (3 + 6) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|server_id|\n",
      "+---------+\n",
      "|      100|\n",
      "|      101|\n",
      "|      102|\n",
      "|      103|\n",
      "|      104|\n",
      "|      105|\n",
      "|      106|\n",
      "|      107|\n",
      "|      108|\n",
      "|      109|\n",
      "|      110|\n",
      "|      111|\n",
      "|      112|\n",
      "|      113|\n",
      "|      114|\n",
      "|      115|\n",
      "|      116|\n",
      "|      117|\n",
      "|      118|\n",
      "|      119|\n",
      "+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# attempt an SQL request with df_server\n",
    "df_count = spark.sql(\n",
    "    \"SELECT DISTINCT server_id \\\n",
    "     FROM utilization \\\n",
    "     ORDER BY server_id\")\n",
    "df_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-------------+\n",
      "|server_id|server_name|session_count|\n",
      "+---------+-----------+-------------+\n",
      "|      133| 133 Server|           68|\n",
      "|      133| 133 Server|           66|\n",
      "|      133| 133 Server|           72|\n",
      "|      133| 133 Server|           77|\n",
      "|      133| 133 Server|           82|\n",
      "|      133| 133 Server|           78|\n",
      "|      133| 133 Server|           68|\n",
      "|      133| 133 Server|           94|\n",
      "|      133| 133 Server|           86|\n",
      "|      133| 133 Server|           92|\n",
      "|      133| 133 Server|           94|\n",
      "|      133| 133 Server|           69|\n",
      "|      133| 133 Server|           88|\n",
      "|      133| 133 Server|           90|\n",
      "|      133| 133 Server|           96|\n",
      "|      133| 133 Server|           66|\n",
      "|      133| 133 Server|           77|\n",
      "|      133| 133 Server|           91|\n",
      "|      133| 133 Server|           84|\n",
      "|      133| 133 Server|           67|\n",
      "+---------+-----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Join between utilization and server_name\n",
    "df_join = spark.sql(\"SELECT u.server_id, sn.server_name, u.session_count \\\n",
    "                     FROM utilization u \\\n",
    "                     INNER JOIN server_name sn \\\n",
    "                     ON sn.server_id = u.server_id\")\n",
    "df_join.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+-------------+\n",
      "|server_name|cpu_utilization|session_count|\n",
      "+-----------+---------------+-------------+\n",
      "| 101 Server|             85|           80|\n",
      "| 101 Server|             80|           90|\n",
      "| 102 Server|             85|           80|\n",
      "| 102 Server|             85|           80|\n",
      "+-----------+---------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# create spark driver node\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark_context = spark.sparkContext\n",
    "\n",
    "# modify spark context\n",
    "df_dup = spark_context.parallelize([Row(server_name='101 Server', cpu_utilization=85, session_count=80),\n",
    "                                    Row(server_name='101 Server',\n",
    "                                        cpu_utilization=80, session_count=90),\n",
    "                                    Row(server_name='102 Server',\n",
    "                                        cpu_utilization=85, session_count=80),\n",
    "                                    Row(server_name='102 Server', cpu_utilization=85, session_count=80)]).toDF()\n",
    "\n",
    "# print out df_dup\n",
    "df_dup.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+-------------+\n",
      "|server_name|cpu_utilization|session_count|\n",
      "+-----------+---------------+-------------+\n",
      "| 101 Server|             85|           80|\n",
      "| 101 Server|             80|           90|\n",
      "| 102 Server|             85|           80|\n",
      "+-----------+---------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# drop exact duplicates\n",
    "df_dup.drop_duplicates().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+-------------+\n",
      "|server_name|cpu_utilization|session_count|\n",
      "+-----------+---------------+-------------+\n",
      "| 101 Server|             85|           80|\n",
      "| 102 Server|             85|           80|\n",
      "+-----------+---------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# drop duplicates in server_name feature\n",
    "df_dup.drop_duplicates(['server_name']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+-------------+\n",
      "|server_name|cpu_utilization|session_count|\n",
      "+-----------+---------------+-------------+\n",
      "| 101 Server|             85|           80|\n",
      "| 101 Server|             80|           90|\n",
      "| 102 Server|             85|           40|\n",
      "| 103 Server|             70|           80|\n",
      "| 104 Server|             60|           80|\n",
      "+-----------+---------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# new dataframe\n",
    "df = spark_context.parallelize([Row(server_name='101 Server', cpu_utilization=85, session_count=80),\n",
    "                                Row(server_name='101 Server',\n",
    "                                    cpu_utilization=80, session_count=90),\n",
    "                                Row(server_name='102 Server',\n",
    "                                    cpu_utilization=85, session_count=40),\n",
    "                                Row(server_name='103 Server',\n",
    "                                    cpu_utilization=70, session_count=80),\n",
    "                                Row(server_name='104 Server', cpu_utilization=60, session_count=80)]).toDF()\n",
    "\n",
    "# print out df\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+-------------+------+\n",
      "|server_name|cpu_utilization|session_count|na_col|\n",
      "+-----------+---------------+-------------+------+\n",
      "| 101 Server|             85|           80|  NULL|\n",
      "| 101 Server|             80|           90|  NULL|\n",
      "| 102 Server|             85|           40|  NULL|\n",
      "| 103 Server|             70|           80|  NULL|\n",
      "| 104 Server|             60|           80|  NULL|\n",
      "+-----------+---------------+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create new column in df \n",
    "df_na = df.withColumn('na_col', lit(None).cast(StringType()))\n",
    "df_na.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+-------------+------+\n",
      "|server_name|cpu_utilization|session_count|na_col|\n",
      "+-----------+---------------+-------------+------+\n",
      "| 101 Server|             85|           80|     A|\n",
      "| 101 Server|             80|           90|     A|\n",
      "| 102 Server|             85|           40|     A|\n",
      "| 103 Server|             70|           80|     A|\n",
      "| 104 Server|             60|           80|     A|\n",
      "+-----------+---------------+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# fill the na with 'A'\n",
    "df_na.fillna('A').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+-------------+------+\n",
      "|server_name|cpu_utilization|session_count|na_col|\n",
      "+-----------+---------------+-------------+------+\n",
      "| 101 Server|             85|           80|     A|\n",
      "| 101 Server|             80|           90|     A|\n",
      "| 102 Server|             85|           40|     A|\n",
      "| 103 Server|             70|           80|     A|\n",
      "| 104 Server|             60|           80|     A|\n",
      "| 101 Server|             85|           80|  NULL|\n",
      "| 101 Server|             80|           90|  NULL|\n",
      "| 102 Server|             85|           40|  NULL|\n",
      "| 103 Server|             70|           80|  NULL|\n",
      "| 104 Server|             60|           80|  NULL|\n",
      "+-----------+---------------+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# union the df_na with the one that's filled\n",
    "df2 = df_na.fillna('A').union(df_na)\n",
    "df2.createOrReplaceTempView(\"na_table\")\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+-------------+------+\n",
      "|server_name|cpu_utilization|session_count|na_col|\n",
      "+-----------+---------------+-------------+------+\n",
      "| 101 Server|             85|           80|     A|\n",
      "| 101 Server|             80|           90|     A|\n",
      "| 102 Server|             85|           40|     A|\n",
      "| 103 Server|             70|           80|     A|\n",
      "| 104 Server|             60|           80|     A|\n",
      "+-----------+---------------+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# drop entries with NULL attributes\n",
    "df2.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+-------------+------+\n",
      "|server_name|cpu_utilization|session_count|na_col|\n",
      "+-----------+---------------+-------------+------+\n",
      "| 101 Server|             85|           80|  NULL|\n",
      "| 101 Server|             80|           90|  NULL|\n",
      "| 102 Server|             85|           40|  NULL|\n",
      "| 103 Server|             70|           80|  NULL|\n",
      "| 104 Server|             60|           80|  NULL|\n",
      "+-----------+---------------+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print out table with NULL in na_col\n",
    "spark.sql(\"SELECT * FROM na_table WHERE na_col IS NULL\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
